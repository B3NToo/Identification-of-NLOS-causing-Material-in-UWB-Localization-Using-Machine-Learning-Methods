{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules from Scikit-learn\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split   # Import train_test_split function\n",
    "from sklearn import metrics   # import metrics modules for accuracy calculation\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification with Reduced Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this experiment, the training will be conducted on 10%, 205, ... , 100% of the data. On each data subset, the model will be trained 10x. <br>\n",
    "For each subset, the mean accuracy will be calculated as well as the standard deviation of the accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline object for the model\n",
    "pipe_MLP = make_pipeline(StandardScaler(),\n",
    "                         MLPClassifier(solver='adam',\n",
    "                                       hidden_layer_sizes=(100,100,100,100), # 3 hidden layers with (100x100x100) neurons\n",
    "                                       random_state=0,\n",
    "                                       max_iter=500\n",
    "                                       )\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the data\n",
    "PATH = \"../../my_data/identification-dataset/my_custom_data/big-identification-dataset.csv\"\n",
    "df = pd.read_csv(PATH)\n",
    "\n",
    "# shuffle the data\n",
    "# df = shuffle(df)\n",
    "\n",
    "# drop NaN values\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data subsets\n",
    "df_10p = df.sample(frac = 0.1, random_state = 200)\n",
    "df_20p = df.sample(frac = 0.2, random_state = 200)\n",
    "df_30p = df.sample(frac = 0.3, random_state = 200)\n",
    "df_40p = df.sample(frac = 0.4, random_state = 200)\n",
    "df_50p = df.sample(frac = 0.5, random_state = 200)\n",
    "df_60p = df.sample(frac = 0.6, random_state = 200)\n",
    "df_70p = df.sample(frac = 0.7, random_state = 200)\n",
    "df_80p = df.sample(frac = 0.8, random_state = 200)\n",
    "df_90p = df.sample(frac = 0.9, random_state = 200)\n",
    "\n",
    "subsets = [df_10p, df_20p, df_30p, df_40p, df_50p, df_60p, df_70p, df_80p, df_90p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [05:08<00:00, 30.83s/it]\n",
      "100%|██████████| 10/10 [19:26<00:00, 116.67s/it]\n",
      "100%|██████████| 10/10 [17:24<00:00, 104.44s/it]\n",
      "100%|██████████| 10/10 [32:45<00:00, 196.55s/it]\n",
      "100%|██████████| 10/10 [55:21<00:00, 332.11s/it]\n",
      "100%|██████████| 10/10 [56:43<00:00, 340.39s/it]\n",
      "100%|██████████| 10/10 [1:27:37<00:00, 525.77s/it]\n",
      "100%|██████████| 10/10 [1:45:58<00:00, 635.88s/it]\n",
      "100%|██████████| 10/10 [1:44:47<00:00, 628.72s/it]\n"
     ]
    }
   ],
   "source": [
    "train_iter = range(10)\n",
    "\n",
    "all_acc = []\n",
    "for d in subsets:\n",
    "    \n",
    "    # Extracted features\n",
    "    X = d[['tdoa', 'snr_an', 'power_dif', 'rx_snr', 'rx_powerdif', 'tof']]\n",
    "    y = d['NLOS_material'] # Labels\n",
    "\n",
    "    # Split dataset into training and test set\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3) # 70% training and 30% test\n",
    "\n",
    "    train_acc = [] # stores all accuracies on a subset\n",
    "    for i in tqdm(train_iter):\n",
    "        # Train the whole pipeline\n",
    "        pipe_MLP.fit(X_train, y_train)\n",
    "\n",
    "        # Make predictions on the test data\n",
    "        y_pred = pipe_MLP.predict(X_test)\n",
    "\n",
    "        # store acc\n",
    "        train_acc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "    \n",
    "    # store the accuracy of all training iteration\n",
    "    all_acc.append(train_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_m =np.array([np.array(xi) for xi in all_acc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9, 10)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_m.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.84052324, 0.84052324, 0.84052324, 0.84052324, 0.84052324,\n",
       "        0.84052324, 0.84052324, 0.84052324, 0.84052324, 0.84052324],\n",
       "       [0.85511482, 0.85511482, 0.85511482, 0.85511482, 0.85511482,\n",
       "        0.85511482, 0.85511482, 0.85511482, 0.85511482, 0.85511482],\n",
       "       [0.85960843, 0.85960843, 0.85960843, 0.85960843, 0.85960843,\n",
       "        0.85960843, 0.85960843, 0.85960843, 0.85960843, 0.85960843],\n",
       "       [0.86485734, 0.86485734, 0.86485734, 0.86485734, 0.86485734,\n",
       "        0.86485734, 0.86485734, 0.86485734, 0.86485734, 0.86485734],\n",
       "       [0.86315555, 0.86315555, 0.86315555, 0.86315555, 0.86315555,\n",
       "        0.86315555, 0.86315555, 0.86315555, 0.86315555, 0.86315555],\n",
       "       [0.86758838, 0.86758838, 0.86758838, 0.86758838, 0.86758838,\n",
       "        0.86758838, 0.86758838, 0.86758838, 0.86758838, 0.86758838],\n",
       "       [0.86566986, 0.86566986, 0.86566986, 0.86566986, 0.86566986,\n",
       "        0.86566986, 0.86566986, 0.86566986, 0.86566986, 0.86566986],\n",
       "       [0.86474825, 0.86474825, 0.86474825, 0.86474825, 0.86474825,\n",
       "        0.86474825, 0.86474825, 0.86474825, 0.86474825, 0.86474825],\n",
       "       [0.85657728, 0.85657728, 0.85657728, 0.85657728, 0.85657728,\n",
       "        0.85657728, 0.85657728, 0.85657728, 0.85657728, 0.85657728]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_m"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dataScience",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
