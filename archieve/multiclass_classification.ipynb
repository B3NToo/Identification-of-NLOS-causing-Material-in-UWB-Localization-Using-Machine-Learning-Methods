{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline MLP Binary Classification Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "metal_1 = \"../../my_data/identification-dataset/nlos/anTag/metal/data1/metal-anTag-data1_data.csv\"\n",
    "metal_2 = \"../../my_data/identification-dataset/nlos/anTag/metal/data2/metal-anTag-data2_data.csv\"\n",
    "metal_3 = \"../../my_data/identification-dataset/nlos/anTag/metal/data3/metal-anTag-data3_data.csv\"\n",
    "\n",
    "foam_1 = \"../../my_data/identification-dataset/nlos/anTag/foam/data1/foam-anTag-data1_data.csv\"\n",
    "foam_2 = \"../../my_data/identification-dataset/nlos/anTag/foam/data2/foam-anTag-data2_data.csv\"\n",
    "foam_3 = \"../../my_data/identification-dataset/nlos/anTag/foam/data3/foam-anTag-data3_data.csv\"\n",
    "\n",
    "plastic_1 = \"../../my_data/identification-dataset/nlos/anTag/plastic/data1/plastic-anTag-data1_data.csv\"\n",
    "plastic_2 = \"../../my_data/identification-dataset/nlos/anTag/plastic/data2/plastic-anTag-data2_data.csv\"\n",
    "plastic_3 = \"../../my_data/identification-dataset/nlos/anTag/plastic/data3/plastic-anTag-data3_data.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metal_1 = pd.read_csv(metal_1)\n",
    "df_metal_2 = pd.read_csv(metal_2)\n",
    "df_metal_3 = pd.read_csv(metal_3)\n",
    "\n",
    "df_foam_1 = pd.read_csv(foam_1)\n",
    "df_foam_2 = pd.read_csv(foam_2)\n",
    "df_foam_3 = pd.read_csv(foam_3)\n",
    "\n",
    "df_plastic_1 = pd.read_csv(plastic_1)\n",
    "df_plastic_2 = pd.read_csv(plastic_2)\n",
    "df_plastic_3 = pd.read_csv(plastic_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting in anchor information\n",
    "df_metal_1_an1 = df_metal_1[['tdoa12', 'snr_an1', 'power_dif_an1', 'an1_rx_snr', 'an1_rx_powerdif', 'an1_tof']].copy()\n",
    "df_metal_1_an2 = df_metal_1[['tdoa21', 'snr_an2', 'power_dif_an2', 'an2_rx_snr', 'an2_rx_powerdif', 'an2_tof']].copy()\n",
    "\n",
    "df_metal_2_an1 = df_metal_2[['tdoa12', 'snr_an1', 'power_dif_an1', 'an1_rx_snr', 'an1_rx_powerdif', 'an1_tof']].copy()\n",
    "df_metal_2_an2 = df_metal_2[['tdoa21', 'snr_an2', 'power_dif_an2', 'an2_rx_snr', 'an2_rx_powerdif', 'an2_tof']].copy()\n",
    "\n",
    "df_metal_3_an1 = df_metal_3[['tdoa12', 'snr_an1', 'power_dif_an1', 'an1_rx_snr', 'an1_rx_powerdif', 'an1_tof']].copy()\n",
    "df_metal_3_an2 = df_metal_3[['tdoa21', 'snr_an2', 'power_dif_an2', 'an2_rx_snr', 'an2_rx_powerdif', 'an2_tof']].copy()\n",
    "\n",
    "\n",
    "df_foam_1_an1 = df_foam_1[['tdoa12', 'snr_an1', 'power_dif_an1', 'an1_rx_snr', 'an1_rx_powerdif', 'an1_tof']].copy()\n",
    "df_foam_1_an2 = df_foam_1[['tdoa21', 'snr_an2', 'power_dif_an2', 'an2_rx_snr', 'an2_rx_powerdif', 'an2_tof']].copy()\n",
    "\n",
    "df_foam_2_an1 = df_foam_2[['tdoa12', 'snr_an1', 'power_dif_an1', 'an1_rx_snr', 'an1_rx_powerdif', 'an1_tof']].copy()\n",
    "df_foam_2_an2 = df_foam_2[['tdoa21', 'snr_an2', 'power_dif_an2', 'an2_rx_snr', 'an2_rx_powerdif', 'an2_tof']].copy()\n",
    "\n",
    "df_foam_3_an1 = df_foam_3[['tdoa12', 'snr_an1', 'power_dif_an1', 'an1_rx_snr', 'an1_rx_powerdif', 'an1_tof']].copy()\n",
    "df_foam_3_an2 = df_foam_3[['tdoa21', 'snr_an2', 'power_dif_an2', 'an2_rx_snr', 'an2_rx_powerdif', 'an2_tof']].copy()\n",
    "\n",
    "\n",
    "df_plastic_1_an1 = df_plastic_1[['tdoa12', 'snr_an1', 'power_dif_an1', 'an1_rx_snr', 'an1_rx_powerdif', 'an1_tof']].copy()\n",
    "df_plastic_1_an2 = df_plastic_1[['tdoa21', 'snr_an2', 'power_dif_an2', 'an2_rx_snr', 'an2_rx_powerdif', 'an2_tof']].copy()\n",
    "\n",
    "df_plastic_2_an1 = df_plastic_2[['tdoa12', 'snr_an1', 'power_dif_an1', 'an1_rx_snr', 'an1_rx_powerdif', 'an1_tof']].copy()\n",
    "df_plastic_2_an2 = df_plastic_2[['tdoa21', 'snr_an2', 'power_dif_an2', 'an2_rx_snr', 'an2_rx_powerdif', 'an2_tof']].copy()\n",
    "\n",
    "df_plastic_3_an1 = df_plastic_3[['tdoa12', 'snr_an1', 'power_dif_an1', 'an1_rx_snr', 'an1_rx_powerdif', 'an1_tof']].copy()\n",
    "df_plastic_3_an2 = df_plastic_3[['tdoa21', 'snr_an2', 'power_dif_an2', 'an2_rx_snr', 'an2_rx_powerdif', 'an2_tof']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "an1_list = [df_metal_1_an1, df_metal_2_an1, df_metal_3_an1,\n",
    "            df_foam_1_an1, df_foam_2_an1, df_foam_3_an1,\n",
    "            df_plastic_1_an1, df_plastic_2_an1, df_plastic_3_an1]\n",
    "\n",
    "an2_list = [df_metal_1_an2, df_metal_2_an2, df_metal_3_an2,\n",
    "            df_foam_1_an2, df_foam_2_an2, df_foam_3_an2,\n",
    "            df_plastic_1_an2, df_plastic_2_an2, df_plastic_3_an2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in an1_list:\n",
    "    df.rename({'tdoa12':'tdoa', \n",
    "                'snr_an1':'snr_an', \n",
    "                'power_dif_an1':'power_dif', \n",
    "                'an1_rx_snr':'rx_snr', \n",
    "                'an1_rx_powerdif':'rx_powerdif', \n",
    "                'an1_tof':'tof'}, axis=1, inplace=True)\n",
    "    \n",
    "for df in an2_list:\n",
    "    df.rename({'tdoa21':'tdoa',\n",
    "                'snr_an2':'snr_an', \n",
    "                'power_dif_an2':'power_dif', \n",
    "                'an2_rx_snr':'rx_snr', \n",
    "                'an2_rx_powerdif':'rx_powerdif', \n",
    "                'an2_tof':'tof'}, axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metal_1_an1 = an1_list[0]\n",
    "df_metal_2_an1 = an1_list[1]\n",
    "df_metal_3_an1 = an1_list[2]\n",
    "df_foam_1_an1 = an1_list[3]\n",
    "df_foam_2_an1 = an1_list[4]\n",
    "df_foam_3_an1 = an1_list[5]\n",
    "df_plastic_1_an1 = an1_list[6]\n",
    "df_plastic_2_an1 = an1_list[7]\n",
    "df_plastic_3_an1 = an1_list[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metal_1_an2 = an2_list[0]\n",
    "df_metal_2_an2 = an2_list[1]\n",
    "df_metal_3_an2 = an2_list[2]\n",
    "df_foam_1_an2 = an2_list[3]\n",
    "df_foam_2_an2 = an2_list[4]\n",
    "df_foam_3_an2 = an2_list[5]\n",
    "df_plastic_1_an2 = an2_list[6]\n",
    "df_plastic_2_an2 = an2_list[7]\n",
    "df_plastic_3_an2 = an2_list[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metal_an1 = pd.concat([df_metal_1_an1, df_metal_2_an1, df_metal_3_an1], ignore_index=True, sort=False)\n",
    "df_metal_an2 = pd.concat([df_metal_1_an1, df_metal_2_an1, df_metal_3_an2], ignore_index=True, sort=False)\n",
    "\n",
    "df_foam_an1 = pd.concat([df_foam_1_an1, df_foam_2_an1, df_foam_3_an1], ignore_index=True, sort=False)\n",
    "df_foam_an2 = pd.concat([df_foam_1_an1, df_foam_2_an1, df_foam_3_an2], ignore_index=True, sort=False)\n",
    "\n",
    "df_plastic_an1 = pd.concat([df_plastic_1_an1, df_plastic_2_an1, df_plastic_3_an1], ignore_index=True, sort=False)\n",
    "df_plastic_an2 = pd.concat([df_plastic_1_an1, df_plastic_2_an1, df_plastic_3_an2], ignore_index=True, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metal_an1['NLOS_material'] = 1 # metal\n",
    "df_metal_an2['NLOS_material'] = 0 # nothing\n",
    "\n",
    "df_foam_an1['NLOS_material'] = 2 # foam\n",
    "df_foam_an2['NLOS_material'] = 0 # nothing\n",
    "\n",
    "df_plastic_an1['NLOS_material'] = 3 # plastic\n",
    "df_plastic_an2['NLOS_material'] = 0 # nothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df_metal_an1, df_metal_an2, df_foam_an1, df_foam_an2, df_plastic_an1, df_plastic_an2], ignore_index=True, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tdoa</th>\n",
       "      <th>snr_an</th>\n",
       "      <th>power_dif</th>\n",
       "      <th>rx_snr</th>\n",
       "      <th>rx_powerdif</th>\n",
       "      <th>tof</th>\n",
       "      <th>NLOS_material</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.319040</td>\n",
       "      <td>24.450001</td>\n",
       "      <td>23.648659</td>\n",
       "      <td>132.000000</td>\n",
       "      <td>14.551033</td>\n",
       "      <td>5.037270</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.319040</td>\n",
       "      <td>24.450001</td>\n",
       "      <td>23.648659</td>\n",
       "      <td>172.666672</td>\n",
       "      <td>14.281364</td>\n",
       "      <td>5.056036</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.319040</td>\n",
       "      <td>14.928572</td>\n",
       "      <td>25.917366</td>\n",
       "      <td>203.149994</td>\n",
       "      <td>14.264801</td>\n",
       "      <td>5.088879</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.464485</td>\n",
       "      <td>20.607143</td>\n",
       "      <td>26.443871</td>\n",
       "      <td>159.000000</td>\n",
       "      <td>13.419594</td>\n",
       "      <td>5.084187</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.300273</td>\n",
       "      <td>26.416666</td>\n",
       "      <td>25.613457</td>\n",
       "      <td>226.899994</td>\n",
       "      <td>13.669937</td>\n",
       "      <td>5.060728</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       tdoa     snr_an  power_dif      rx_snr  rx_powerdif       tof  \\\n",
       "0 -0.319040  24.450001  23.648659  132.000000    14.551033  5.037270   \n",
       "1 -0.319040  24.450001  23.648659  172.666672    14.281364  5.056036   \n",
       "2 -0.319040  14.928572  25.917366  203.149994    14.264801  5.088879   \n",
       "3 -0.464485  20.607143  26.443871  159.000000    13.419594  5.084187   \n",
       "4 -0.300273  26.416666  25.613457  226.899994    13.669937  5.060728   \n",
       "\n",
       "   NLOS_material  \n",
       "0              1  \n",
       "1              1  \n",
       "2              1  \n",
       "3              1  \n",
       "4              1  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 2, 3])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['NLOS_material'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tdoa</th>\n",
       "      <th>snr_an</th>\n",
       "      <th>power_dif</th>\n",
       "      <th>rx_snr</th>\n",
       "      <th>rx_powerdif</th>\n",
       "      <th>tof</th>\n",
       "      <th>NLOS_material</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>35936.000000</td>\n",
       "      <td>35936.000000</td>\n",
       "      <td>35936.000000</td>\n",
       "      <td>35936.000000</td>\n",
       "      <td>35936.000000</td>\n",
       "      <td>35936.000000</td>\n",
       "      <td>35936.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-0.197316</td>\n",
       "      <td>98.292288</td>\n",
       "      <td>15.065376</td>\n",
       "      <td>197.675978</td>\n",
       "      <td>11.475853</td>\n",
       "      <td>5.051496</td>\n",
       "      <td>1.002087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.380310</td>\n",
       "      <td>54.038605</td>\n",
       "      <td>6.341710</td>\n",
       "      <td>30.943111</td>\n",
       "      <td>1.580808</td>\n",
       "      <td>0.021468</td>\n",
       "      <td>1.156212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-9.580582</td>\n",
       "      <td>6.208333</td>\n",
       "      <td>-6.842743</td>\n",
       "      <td>5.015790</td>\n",
       "      <td>-31.256706</td>\n",
       "      <td>4.976276</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.342499</td>\n",
       "      <td>25.785715</td>\n",
       "      <td>10.934319</td>\n",
       "      <td>176.649994</td>\n",
       "      <td>10.372650</td>\n",
       "      <td>5.037270</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-0.201746</td>\n",
       "      <td>113.218750</td>\n",
       "      <td>12.311340</td>\n",
       "      <td>196.708328</td>\n",
       "      <td>10.960880</td>\n",
       "      <td>5.051345</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>-0.126678</td>\n",
       "      <td>137.583328</td>\n",
       "      <td>23.820045</td>\n",
       "      <td>221.199997</td>\n",
       "      <td>12.727268</td>\n",
       "      <td>5.065420</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4.077143</td>\n",
       "      <td>249.649994</td>\n",
       "      <td>32.333527</td>\n",
       "      <td>320.312500</td>\n",
       "      <td>15.155426</td>\n",
       "      <td>5.145180</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               tdoa        snr_an     power_dif        rx_snr   rx_powerdif  \\\n",
       "count  35936.000000  35936.000000  35936.000000  35936.000000  35936.000000   \n",
       "mean      -0.197316     98.292288     15.065376    197.675978     11.475853   \n",
       "std        0.380310     54.038605      6.341710     30.943111      1.580808   \n",
       "min       -9.580582      6.208333     -6.842743      5.015790    -31.256706   \n",
       "25%       -0.342499     25.785715     10.934319    176.649994     10.372650   \n",
       "50%       -0.201746    113.218750     12.311340    196.708328     10.960880   \n",
       "75%       -0.126678    137.583328     23.820045    221.199997     12.727268   \n",
       "max        4.077143    249.649994     32.333527    320.312500     15.155426   \n",
       "\n",
       "                tof  NLOS_material  \n",
       "count  35936.000000   35936.000000  \n",
       "mean       5.051496       1.002087  \n",
       "std        0.021468       1.156212  \n",
       "min        4.976276       0.000000  \n",
       "25%        5.037270       0.000000  \n",
       "50%        5.051345       0.500000  \n",
       "75%        5.065420       2.000000  \n",
       "max        5.145180       3.000000  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules from Scikit-learn\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split   # Import train_test_split function\n",
    "from sklearn import metrics   # import metrics modules for accuracy calculation\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline object for the model\n",
    "pipe_MLP = make_pipeline(StandardScaler(),\n",
    "                         MLPClassifier(solver='adam',\n",
    "                                       hidden_layer_sizes=(100,100,100,), # 3 hidden layers with (100x100x100) neurons\n",
    "                                       random_state=0,\n",
    "                                       max_iter=500,           # TODO: tune it later\n",
    "                                       verbose=True\n",
    "                                       )\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracted features \n",
    "X = df[['tdoa', 'snr_an', 'power_dif', 'rx_snr', 'rx_powerdif', 'tof']]\n",
    "y = df['NLOS_material'] # Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into training and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3) # 70% training and 30% test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.84847648\n",
      "Iteration 2, loss = 0.71967942\n",
      "Iteration 3, loss = 0.68837482\n",
      "Iteration 4, loss = 0.67301790\n",
      "Iteration 5, loss = 0.65787659\n",
      "Iteration 6, loss = 0.65111226\n",
      "Iteration 7, loss = 0.64601508\n",
      "Iteration 8, loss = 0.63891627\n",
      "Iteration 9, loss = 0.63496615\n",
      "Iteration 10, loss = 0.62964070\n",
      "Iteration 11, loss = 0.62903154\n",
      "Iteration 12, loss = 0.62409444\n",
      "Iteration 13, loss = 0.62171258\n",
      "Iteration 14, loss = 0.61978378\n",
      "Iteration 15, loss = 0.61960932\n",
      "Iteration 16, loss = 0.61974225\n",
      "Iteration 17, loss = 0.61548146\n",
      "Iteration 18, loss = 0.61565126\n",
      "Iteration 19, loss = 0.61389825\n",
      "Iteration 20, loss = 0.61239187\n",
      "Iteration 21, loss = 0.61060744\n",
      "Iteration 22, loss = 0.61087927\n",
      "Iteration 23, loss = 0.61134975\n",
      "Iteration 24, loss = 0.60886442\n",
      "Iteration 25, loss = 0.60786921\n",
      "Iteration 26, loss = 0.60847064\n",
      "Iteration 27, loss = 0.60723993\n",
      "Iteration 28, loss = 0.60683866\n",
      "Iteration 29, loss = 0.60644201\n",
      "Iteration 30, loss = 0.60657870\n",
      "Iteration 31, loss = 0.60674212\n",
      "Iteration 32, loss = 0.60651701\n",
      "Iteration 33, loss = 0.60553744\n",
      "Iteration 34, loss = 0.60587468\n",
      "Iteration 35, loss = 0.60396687\n",
      "Iteration 36, loss = 0.60506469\n",
      "Iteration 37, loss = 0.60332678\n",
      "Iteration 38, loss = 0.60243279\n",
      "Iteration 39, loss = 0.60275112\n",
      "Iteration 40, loss = 0.60196590\n",
      "Iteration 41, loss = 0.60114556\n",
      "Iteration 42, loss = 0.60358167\n",
      "Iteration 43, loss = 0.60057121\n",
      "Iteration 44, loss = 0.59976378\n",
      "Iteration 45, loss = 0.60281287\n",
      "Iteration 46, loss = 0.60053311\n",
      "Iteration 47, loss = 0.59962728\n",
      "Iteration 48, loss = 0.59928694\n",
      "Iteration 49, loss = 0.59928295\n",
      "Iteration 50, loss = 0.59944514\n",
      "Iteration 51, loss = 0.59987120\n",
      "Iteration 52, loss = 0.59846164\n",
      "Iteration 53, loss = 0.59804380\n",
      "Iteration 54, loss = 0.59663876\n",
      "Iteration 55, loss = 0.59838045\n",
      "Iteration 56, loss = 0.59693227\n",
      "Iteration 57, loss = 0.59581902\n",
      "Iteration 58, loss = 0.59627070\n",
      "Iteration 59, loss = 0.59580825\n",
      "Iteration 60, loss = 0.59730836\n",
      "Iteration 61, loss = 0.59579753\n",
      "Iteration 62, loss = 0.59561017\n",
      "Iteration 63, loss = 0.59461273\n",
      "Iteration 64, loss = 0.59421421\n",
      "Iteration 65, loss = 0.59375794\n",
      "Iteration 66, loss = 0.59359454\n",
      "Iteration 67, loss = 0.59309200\n",
      "Iteration 68, loss = 0.59354259\n",
      "Iteration 69, loss = 0.59248349\n",
      "Iteration 70, loss = 0.59321131\n",
      "Iteration 71, loss = 0.59164223\n",
      "Iteration 72, loss = 0.59307210\n",
      "Iteration 73, loss = 0.59354174\n",
      "Iteration 74, loss = 0.59201766\n",
      "Iteration 75, loss = 0.59155946\n",
      "Iteration 76, loss = 0.59121982\n",
      "Iteration 77, loss = 0.59186061\n",
      "Iteration 78, loss = 0.59096069\n",
      "Iteration 79, loss = 0.59125729\n",
      "Iteration 80, loss = 0.58967241\n",
      "Iteration 81, loss = 0.59008041\n",
      "Iteration 82, loss = 0.58943770\n",
      "Iteration 83, loss = 0.58984264\n",
      "Iteration 84, loss = 0.59015881\n",
      "Iteration 85, loss = 0.58829753\n",
      "Iteration 86, loss = 0.58894662\n",
      "Iteration 87, loss = 0.58833477\n",
      "Iteration 88, loss = 0.58768225\n",
      "Iteration 89, loss = 0.58821745\n",
      "Iteration 90, loss = 0.58743126\n",
      "Iteration 91, loss = 0.58841504\n",
      "Iteration 92, loss = 0.58702260\n",
      "Iteration 93, loss = 0.58711960\n",
      "Iteration 94, loss = 0.58542997\n",
      "Iteration 95, loss = 0.58670670\n",
      "Iteration 96, loss = 0.58462591\n",
      "Iteration 97, loss = 0.58565400\n",
      "Iteration 98, loss = 0.58548261\n",
      "Iteration 99, loss = 0.58532470\n",
      "Iteration 100, loss = 0.58520709\n",
      "Iteration 101, loss = 0.58502541\n",
      "Iteration 102, loss = 0.58435844\n",
      "Iteration 103, loss = 0.58338191\n",
      "Iteration 104, loss = 0.58271757\n",
      "Iteration 105, loss = 0.58313810\n",
      "Iteration 106, loss = 0.58225091\n",
      "Iteration 107, loss = 0.58211106\n",
      "Iteration 108, loss = 0.58414866\n",
      "Iteration 109, loss = 0.58208945\n",
      "Iteration 110, loss = 0.58155568\n",
      "Iteration 111, loss = 0.57992313\n",
      "Iteration 112, loss = 0.58251524\n",
      "Iteration 113, loss = 0.58009747\n",
      "Iteration 114, loss = 0.58147008\n",
      "Iteration 115, loss = 0.57975502\n",
      "Iteration 116, loss = 0.58112055\n",
      "Iteration 117, loss = 0.58049603\n",
      "Iteration 118, loss = 0.57987475\n",
      "Iteration 119, loss = 0.57912416\n",
      "Iteration 120, loss = 0.57900270\n",
      "Iteration 121, loss = 0.57946195\n",
      "Iteration 122, loss = 0.57853746\n",
      "Iteration 123, loss = 0.57834949\n",
      "Iteration 124, loss = 0.57792462\n",
      "Iteration 125, loss = 0.57662566\n",
      "Iteration 126, loss = 0.57734993\n",
      "Iteration 127, loss = 0.57892421\n",
      "Iteration 128, loss = 0.57591775\n",
      "Iteration 129, loss = 0.57680128\n",
      "Iteration 130, loss = 0.57498449\n",
      "Iteration 131, loss = 0.57505514\n",
      "Iteration 132, loss = 0.57601549\n",
      "Iteration 133, loss = 0.57625638\n",
      "Iteration 134, loss = 0.57512697\n",
      "Iteration 135, loss = 0.57497551\n",
      "Iteration 136, loss = 0.57480260\n",
      "Iteration 137, loss = 0.57386364\n",
      "Iteration 138, loss = 0.57478090\n",
      "Iteration 139, loss = 0.57305667\n",
      "Iteration 140, loss = 0.57313475\n",
      "Iteration 141, loss = 0.57390587\n",
      "Iteration 142, loss = 0.57183536\n",
      "Iteration 143, loss = 0.57207968\n",
      "Iteration 144, loss = 0.57325453\n",
      "Iteration 145, loss = 0.57153624\n",
      "Iteration 146, loss = 0.57203458\n",
      "Iteration 147, loss = 0.57161959\n",
      "Iteration 148, loss = 0.56907277\n",
      "Iteration 149, loss = 0.56981233\n",
      "Iteration 150, loss = 0.56929539\n",
      "Iteration 151, loss = 0.57128673\n",
      "Iteration 152, loss = 0.56886057\n",
      "Iteration 153, loss = 0.56928731\n",
      "Iteration 154, loss = 0.56927910\n",
      "Iteration 155, loss = 0.56849755\n",
      "Iteration 156, loss = 0.56928769\n",
      "Iteration 157, loss = 0.56853102\n",
      "Iteration 158, loss = 0.56868691\n",
      "Iteration 159, loss = 0.56863624\n",
      "Iteration 160, loss = 0.56750246\n",
      "Iteration 161, loss = 0.56763125\n",
      "Iteration 162, loss = 0.56622234\n",
      "Iteration 163, loss = 0.56575908\n",
      "Iteration 164, loss = 0.56634837\n",
      "Iteration 165, loss = 0.56513364\n",
      "Iteration 166, loss = 0.56467471\n",
      "Iteration 167, loss = 0.56685453\n",
      "Iteration 168, loss = 0.56581265\n",
      "Iteration 169, loss = 0.56560809\n",
      "Iteration 170, loss = 0.56452347\n",
      "Iteration 171, loss = 0.56504528\n",
      "Iteration 172, loss = 0.56587546\n",
      "Iteration 173, loss = 0.56367509\n",
      "Iteration 174, loss = 0.56325485\n",
      "Iteration 175, loss = 0.56228268\n",
      "Iteration 176, loss = 0.56275504\n",
      "Iteration 177, loss = 0.56263986\n",
      "Iteration 178, loss = 0.56223691\n",
      "Iteration 179, loss = 0.56092068\n",
      "Iteration 180, loss = 0.56062465\n",
      "Iteration 181, loss = 0.56089694\n",
      "Iteration 182, loss = 0.56393799\n",
      "Iteration 183, loss = 0.56072315\n",
      "Iteration 184, loss = 0.56137882\n",
      "Iteration 185, loss = 0.56023812\n",
      "Iteration 186, loss = 0.55993237\n",
      "Iteration 187, loss = 0.55984310\n",
      "Iteration 188, loss = 0.55985677\n",
      "Iteration 189, loss = 0.55936219\n",
      "Iteration 190, loss = 0.55798372\n",
      "Iteration 191, loss = 0.55939481\n",
      "Iteration 192, loss = 0.55805992\n",
      "Iteration 193, loss = 0.55746084\n",
      "Iteration 194, loss = 0.55815518\n",
      "Iteration 195, loss = 0.55786710\n",
      "Iteration 196, loss = 0.55760039\n",
      "Iteration 197, loss = 0.55590531\n",
      "Iteration 198, loss = 0.55583381\n",
      "Iteration 199, loss = 0.55580899\n",
      "Iteration 200, loss = 0.55663836\n",
      "Iteration 201, loss = 0.55572518\n",
      "Iteration 202, loss = 0.55476644\n",
      "Iteration 203, loss = 0.55595268\n",
      "Iteration 204, loss = 0.55351688\n",
      "Iteration 205, loss = 0.55421692\n",
      "Iteration 206, loss = 0.55428533\n",
      "Iteration 207, loss = 0.55413603\n",
      "Iteration 208, loss = 0.55431289\n",
      "Iteration 209, loss = 0.55295785\n",
      "Iteration 210, loss = 0.55439592\n",
      "Iteration 211, loss = 0.55261699\n",
      "Iteration 212, loss = 0.55384284\n",
      "Iteration 213, loss = 0.55226086\n",
      "Iteration 214, loss = 0.55165233\n",
      "Iteration 215, loss = 0.55272758\n",
      "Iteration 216, loss = 0.55130657\n",
      "Iteration 217, loss = 0.55299514\n",
      "Iteration 218, loss = 0.55233210\n",
      "Iteration 219, loss = 0.55115043\n",
      "Iteration 220, loss = 0.55132691\n",
      "Iteration 221, loss = 0.55040731\n",
      "Iteration 222, loss = 0.55058011\n",
      "Iteration 223, loss = 0.54921557\n",
      "Iteration 224, loss = 0.55037459\n",
      "Iteration 225, loss = 0.54975850\n",
      "Iteration 226, loss = 0.54978053\n",
      "Iteration 227, loss = 0.54914135\n",
      "Iteration 228, loss = 0.54971691\n",
      "Iteration 229, loss = 0.54740619\n",
      "Iteration 230, loss = 0.54900305\n",
      "Iteration 231, loss = 0.54636815\n",
      "Iteration 232, loss = 0.54989679\n",
      "Iteration 233, loss = 0.54817254\n",
      "Iteration 234, loss = 0.54685649\n",
      "Iteration 235, loss = 0.54694463\n",
      "Iteration 236, loss = 0.54773682\n",
      "Iteration 237, loss = 0.54627595\n",
      "Iteration 238, loss = 0.54597686\n",
      "Iteration 239, loss = 0.54550361\n",
      "Iteration 240, loss = 0.54638876\n",
      "Iteration 241, loss = 0.54524344\n",
      "Iteration 242, loss = 0.54649213\n",
      "Iteration 243, loss = 0.54668465\n",
      "Iteration 244, loss = 0.54516855\n",
      "Iteration 245, loss = 0.54428025\n",
      "Iteration 246, loss = 0.54435124\n",
      "Iteration 247, loss = 0.54355750\n",
      "Iteration 248, loss = 0.54455652\n",
      "Iteration 249, loss = 0.54573177\n",
      "Iteration 250, loss = 0.54435181\n",
      "Iteration 251, loss = 0.54372419\n",
      "Iteration 252, loss = 0.54355602\n",
      "Iteration 253, loss = 0.54254015\n",
      "Iteration 254, loss = 0.54279036\n",
      "Iteration 255, loss = 0.54206337\n",
      "Iteration 256, loss = 0.54197153\n",
      "Iteration 257, loss = 0.54361449\n",
      "Iteration 258, loss = 0.54225188\n",
      "Iteration 259, loss = 0.54191584\n",
      "Iteration 260, loss = 0.54098863\n",
      "Iteration 261, loss = 0.54023716\n",
      "Iteration 262, loss = 0.53903075\n",
      "Iteration 263, loss = 0.54078413\n",
      "Iteration 264, loss = 0.54033361\n",
      "Iteration 265, loss = 0.53759367\n",
      "Iteration 266, loss = 0.53892753\n",
      "Iteration 267, loss = 0.53895133\n",
      "Iteration 268, loss = 0.53923163\n",
      "Iteration 269, loss = 0.53830327\n",
      "Iteration 270, loss = 0.53861557\n",
      "Iteration 271, loss = 0.53917536\n",
      "Iteration 272, loss = 0.53885996\n",
      "Iteration 273, loss = 0.53721641\n",
      "Iteration 274, loss = 0.53831884\n",
      "Iteration 275, loss = 0.53837409\n",
      "Iteration 276, loss = 0.53638236\n",
      "Iteration 277, loss = 0.53785059\n",
      "Iteration 278, loss = 0.53899734\n",
      "Iteration 279, loss = 0.53695304\n",
      "Iteration 280, loss = 0.53604078\n",
      "Iteration 281, loss = 0.53598389\n",
      "Iteration 282, loss = 0.53627106\n",
      "Iteration 283, loss = 0.53487196\n",
      "Iteration 284, loss = 0.53456002\n",
      "Iteration 285, loss = 0.53532526\n",
      "Iteration 286, loss = 0.53637362\n",
      "Iteration 287, loss = 0.53553439\n",
      "Iteration 288, loss = 0.53461360\n",
      "Iteration 289, loss = 0.53362515\n",
      "Iteration 290, loss = 0.53292976\n",
      "Iteration 291, loss = 0.53685138\n",
      "Iteration 292, loss = 0.53237121\n",
      "Iteration 293, loss = 0.53500277\n",
      "Iteration 294, loss = 0.53300215\n",
      "Iteration 295, loss = 0.53319602\n",
      "Iteration 296, loss = 0.53394322\n",
      "Iteration 297, loss = 0.53240758\n",
      "Iteration 298, loss = 0.53161889\n",
      "Iteration 299, loss = 0.53227867\n",
      "Iteration 300, loss = 0.53332360\n",
      "Iteration 301, loss = 0.53178068\n",
      "Iteration 302, loss = 0.53220753\n",
      "Iteration 303, loss = 0.53111771\n",
      "Iteration 304, loss = 0.53139719\n",
      "Iteration 305, loss = 0.53248227\n",
      "Iteration 306, loss = 0.53131565\n",
      "Iteration 307, loss = 0.53178644\n",
      "Iteration 308, loss = 0.53078605\n",
      "Iteration 309, loss = 0.52999881\n",
      "Iteration 310, loss = 0.53085824\n",
      "Iteration 311, loss = 0.53281797\n",
      "Iteration 312, loss = 0.53048574\n",
      "Iteration 313, loss = 0.52871102\n",
      "Iteration 314, loss = 0.53049637\n",
      "Iteration 315, loss = 0.52930508\n",
      "Iteration 316, loss = 0.52933189\n",
      "Iteration 317, loss = 0.52880266\n",
      "Iteration 318, loss = 0.52741360\n",
      "Iteration 319, loss = 0.52721457\n",
      "Iteration 320, loss = 0.52784404\n",
      "Iteration 321, loss = 0.53022864\n",
      "Iteration 322, loss = 0.52940456\n",
      "Iteration 323, loss = 0.52926509\n",
      "Iteration 324, loss = 0.52881964\n",
      "Iteration 325, loss = 0.52869693\n",
      "Iteration 326, loss = 0.52779751\n",
      "Iteration 327, loss = 0.52983405\n",
      "Iteration 328, loss = 0.52870108\n",
      "Iteration 329, loss = 0.52824346\n",
      "Iteration 330, loss = 0.52560219\n",
      "Iteration 331, loss = 0.52593505\n",
      "Iteration 332, loss = 0.52653310\n",
      "Iteration 333, loss = 0.52681758\n",
      "Iteration 334, loss = 0.52622223\n",
      "Iteration 335, loss = 0.52540513\n",
      "Iteration 336, loss = 0.52649705\n",
      "Iteration 337, loss = 0.52420649\n",
      "Iteration 338, loss = 0.52634718\n",
      "Iteration 339, loss = 0.52459375\n",
      "Iteration 340, loss = 0.52526171\n",
      "Iteration 341, loss = 0.52440584\n",
      "Iteration 342, loss = 0.52345125\n",
      "Iteration 343, loss = 0.52415401\n",
      "Iteration 344, loss = 0.52393755\n",
      "Iteration 345, loss = 0.52333073\n",
      "Iteration 346, loss = 0.52421522\n",
      "Iteration 347, loss = 0.52395014\n",
      "Iteration 348, loss = 0.52175210\n",
      "Iteration 349, loss = 0.52256609\n",
      "Iteration 350, loss = 0.52327826\n",
      "Iteration 351, loss = 0.52162554\n",
      "Iteration 352, loss = 0.52304240\n",
      "Iteration 353, loss = 0.52254453\n",
      "Iteration 354, loss = 0.52231823\n",
      "Iteration 355, loss = 0.52102065\n",
      "Iteration 356, loss = 0.52176765\n",
      "Iteration 357, loss = 0.52266436\n",
      "Iteration 358, loss = 0.52140414\n",
      "Iteration 359, loss = 0.52260954\n",
      "Iteration 360, loss = 0.52266580\n",
      "Iteration 361, loss = 0.52035862\n",
      "Iteration 362, loss = 0.52025109\n",
      "Iteration 363, loss = 0.52181431\n",
      "Iteration 364, loss = 0.52120225\n",
      "Iteration 365, loss = 0.52092127\n",
      "Iteration 366, loss = 0.51780269\n",
      "Iteration 367, loss = 0.52056450\n",
      "Iteration 368, loss = 0.52405149\n",
      "Iteration 369, loss = 0.52019749\n",
      "Iteration 370, loss = 0.51922611\n",
      "Iteration 371, loss = 0.51850020\n",
      "Iteration 372, loss = 0.52011845\n",
      "Iteration 373, loss = 0.51825645\n",
      "Iteration 374, loss = 0.51842063\n",
      "Iteration 375, loss = 0.51810430\n",
      "Iteration 376, loss = 0.51703099\n",
      "Iteration 377, loss = 0.51808577\n",
      "Iteration 378, loss = 0.51891851\n",
      "Iteration 379, loss = 0.51722821\n",
      "Iteration 380, loss = 0.51884849\n",
      "Iteration 381, loss = 0.51661684\n",
      "Iteration 382, loss = 0.51795440\n",
      "Iteration 383, loss = 0.51572870\n",
      "Iteration 384, loss = 0.51715315\n",
      "Iteration 385, loss = 0.51749873\n",
      "Iteration 386, loss = 0.51692491\n",
      "Iteration 387, loss = 0.51717583\n",
      "Iteration 388, loss = 0.51676688\n",
      "Iteration 389, loss = 0.51846494\n",
      "Iteration 390, loss = 0.51650971\n",
      "Iteration 391, loss = 0.51832287\n",
      "Iteration 392, loss = 0.51700234\n",
      "Iteration 393, loss = 0.51553508\n",
      "Iteration 394, loss = 0.51807108\n",
      "Iteration 395, loss = 0.51511629\n",
      "Iteration 396, loss = 0.51567706\n",
      "Iteration 397, loss = 0.51640455\n",
      "Iteration 398, loss = 0.51502153\n",
      "Iteration 399, loss = 0.51433184\n",
      "Iteration 400, loss = 0.51613872\n",
      "Iteration 401, loss = 0.51532358\n",
      "Iteration 402, loss = 0.51473343\n",
      "Iteration 403, loss = 0.51614190\n",
      "Iteration 404, loss = 0.51305543\n",
      "Iteration 405, loss = 0.51454161\n",
      "Iteration 406, loss = 0.51370208\n",
      "Iteration 407, loss = 0.51417799\n",
      "Iteration 408, loss = 0.51252437\n",
      "Iteration 409, loss = 0.51381842\n",
      "Iteration 410, loss = 0.51263475\n",
      "Iteration 411, loss = 0.51313845\n",
      "Iteration 412, loss = 0.51259683\n",
      "Iteration 413, loss = 0.51277653\n",
      "Iteration 414, loss = 0.51349207\n",
      "Iteration 415, loss = 0.51127915\n",
      "Iteration 416, loss = 0.51394394\n",
      "Iteration 417, loss = 0.51126573\n",
      "Iteration 418, loss = 0.51255189\n",
      "Iteration 419, loss = 0.51161489\n",
      "Iteration 420, loss = 0.51243606\n",
      "Iteration 421, loss = 0.51144545\n",
      "Iteration 422, loss = 0.51102722\n",
      "Iteration 423, loss = 0.51043960\n",
      "Iteration 424, loss = 0.51116599\n",
      "Iteration 425, loss = 0.51173748\n",
      "Iteration 426, loss = 0.51175638\n",
      "Iteration 427, loss = 0.51329144\n",
      "Iteration 428, loss = 0.51474821\n",
      "Iteration 429, loss = 0.51250979\n",
      "Iteration 430, loss = 0.51007139\n",
      "Iteration 431, loss = 0.51081886\n",
      "Iteration 432, loss = 0.50901483\n",
      "Iteration 433, loss = 0.50879680\n",
      "Iteration 434, loss = 0.50967866\n",
      "Iteration 435, loss = 0.51069964\n",
      "Iteration 436, loss = 0.50831666\n",
      "Iteration 437, loss = 0.50807996\n",
      "Iteration 438, loss = 0.50953111\n",
      "Iteration 439, loss = 0.50968523\n",
      "Iteration 440, loss = 0.50921375\n",
      "Iteration 441, loss = 0.51048637\n",
      "Iteration 442, loss = 0.50734151\n",
      "Iteration 443, loss = 0.50873530\n",
      "Iteration 444, loss = 0.50903446\n",
      "Iteration 445, loss = 0.50810695\n",
      "Iteration 446, loss = 0.50785832\n",
      "Iteration 447, loss = 0.50767225\n",
      "Iteration 448, loss = 0.50752260\n",
      "Iteration 449, loss = 0.50796677\n",
      "Iteration 450, loss = 0.50929945\n",
      "Iteration 451, loss = 0.50831860\n",
      "Iteration 452, loss = 0.50612468\n",
      "Iteration 453, loss = 0.50705683\n",
      "Iteration 454, loss = 0.50722604\n",
      "Iteration 455, loss = 0.50694405\n",
      "Iteration 456, loss = 0.50716715\n",
      "Iteration 457, loss = 0.50758563\n",
      "Iteration 458, loss = 0.50639428\n",
      "Iteration 459, loss = 0.50798689\n",
      "Iteration 460, loss = 0.50728542\n",
      "Iteration 461, loss = 0.50641704\n",
      "Iteration 462, loss = 0.50691352\n",
      "Iteration 463, loss = 0.50670189\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('standardscaler', StandardScaler()),\n",
       "                ('mlpclassifier',\n",
       "                 MLPClassifier(hidden_layer_sizes=(100, 100, 100), max_iter=500,\n",
       "                               random_state=0, verbose=True))])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the whole pipeline\n",
    "pipe_MLP.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test data\n",
    "y_pred = pipe_MLP.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 52.36063444949448%\n"
     ]
    }
   ],
   "source": [
    "# Caluclate the accuracy on test data predicitons\n",
    "print(f'Test Accuracy: {metrics.accuracy_score(y_test, y_pred) * 100}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dataScience",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
